# TODO
- [x] Build basic `Scheduler`
- [x] Write default `Scheduler.getUrlFingerprint` with MetroHash
- [x] Make both scraping and data processing tasks asynchronous and independent of each other
- [ ] Filter duplicate URLs with bloom filter
- [ ] Build `Scraper`
- [ ] Build `DataProcessor`
- [ ] Use Redis
- [ ] Handle failed results
- [ ] Logging
- [ ] Multithreading/multiprocessing crawling tasks?
